
#  الگوریتم  Naive Bayes



## ۱. مقدمه
الگوریتم **Naive Bayes** یکی از ساده‌ترین و در عین حال قدرتمندترین الگوریتم‌های **طبقه‌بندی (Classification)** در یادگیری ماشینی است.  
این الگوریتم بر پایه **قضیه بیز (Bayes’ Theorem)** ساخته شده و برای پیش‌بینی کلاس‌ها بر اساس ویژگی‌های داده‌ها استفاده می‌شود.

### کاربردها:
- تشخیص اسپم (Spam Detection)  
- تحلیل احساسات (Sentiment Analysis)  
- تشخیص موضوع متن (Text Categorization)  
- پیش‌بینی پزشکی (Diagnosis)  

**هدف اصلی:**  
به‌روزرسانی احتمال یک فرض یا کلاس با مشاهده داده‌های جدید.

---

## ۲. قضیه بیز (Bayes’ Theorem) 

فرمول اصلی:

$\[
P(H|E) = \frac{P(E|H) \cdot P(H)}{P(E)}
\]$

### معنی اصطلاحات

- **`P(H | E)`**  احتمال پسین (Posterior):

  احتمال درست بودن فرض `H` بعد از مشاهده شواهد `E`

- **`P(E | H)`**  درست‌نمایی (Likelihood):
  
  
 احتمال مشاهده `E` در صورتی که فرض `H` درست باشد

- **`P(H)`**  احتمال پیشین (Prior):
    
احتمال اولیه فرض `H` قبل از دیدن داده‌ها

- **`P(E)`**  شواهد (Evidence) :
   
احتمال مشاهده داده‌ها در کل فضای نمونه


---

### مثال عملی: تشخیص بیماری نادر

فرض کنید بیماری X در جمعیت نادر است (۰.۵٪ افراد بیمارند):

- \( P(H) = 0.005 \)  

دقت تست:  
  - بیمار → مثبت ۹۵٪ → \( P(E|H) = 0.95 \)  
  - سالم → مثبت ۱۰٪ → \( P(E|-H) = 0.10 \)  

**سوال:** اگر تست مثبت شود، احتمال واقعی بیمار بودن چقدر است؟  

#### مرحله ۱: محاسبه احتمال شواهد \( P(E) \)

$\[
P(E) = P(E|H) \cdot P(H) + P(E|\neg H) \cdot P(\neg H)
\]$

$\[
P(E) = (0.95 \cdot 0.005) + (0.10 \cdot 0.995) = 0.00475 + 0.0995 = 0.10425
\]$

#### مرحله ۲: محاسبه احتمال پسین \( P(H|E) \)

$\[
P(H|E) = \frac{0.95 \cdot 0.005}{0.10425} \approx 0.0456 \approx 4.56\%
\]$

> حتی با تست مثبت، احتمال واقعی بیمار بودن تنها حدود ۴.۵٪ است!  
> دلیل: بیماری نادر است و مثبت کاذب وجود دارد.

---

## ۳.اصول و فرمول‌ها Naive Bayes  

فرض کنید:

- کلاس‌ها: \( C_1, C_2, ..., C_k \)  
- ویژگی‌ها: \( x_1, x_2, ..., x_n \)

**هدف:** پیدا کردن کلاس با بیشترین احتمال پسین:

$\[
\hat{y} = \arg\max_{C_k} P(C_k | x_1, ..., x_n)
\]$

با قضیه بیز:

$\[
P(C_k | x_1, ..., x_n) = \frac{P(C_k) \cdot P(x_1, ..., x_n | C_k)}{P(x_1, ..., x_n)}
\]$

چون \( P(x_1, ..., x_n) \) برای همه کلاس‌ها یکسان است:

$\[
\hat{y} = \arg\max_{C_k} P(C_k) \cdot P(x_1, ..., x_n | C_k)
\]$

### فرض "Naive"

فرض می‌کنیم ویژگی‌ها **مستقل هستند**:

$\[
P(x_1, ..., x_n | C_k) = \prod_{i=1}^n P(x_i | C_k)
\]$

---

## ۴. انواع Naive Bayes (مرحله به مرحله)

### ۴.۱ الگوریتم Gaussian Naive Bayes
- برای داده‌های **پیوسته** مثل قد، وزن، نمره  
- فرض توزیع **نرمال (Gaussian)**:

$\[
P(x_i | C_k) = \frac{1}{\sqrt{2 \pi \sigma_k^2}} \exp\left(-\frac{(x_i - \mu_k)^2}{2\sigma_k^2}\right)
\]$

**مثال عملی:**  
پیش‌بینی قبولی دانش‌آموز بر اساس ساعت مطالعه و نمره میان‌ترم:

| کلاس | ساعت مطالعه | نمره میان‌ترم |
|------|------------|----------------|
| قبول | 4, 5, 6 | 70, 75, 80 |
| مردود | 1, 2, 3 | 40, 45, 50 |

برای نمونه جدید (ساعت مطالعه=5، نمره=78) → محاسبه \( P(x_i|C_k) \) و ضرب با Prior → انتخاب کلاس.

---

### ۴.۲ الگوریتم Multinomial Naive Bayes
- برای داده‌های **گسسته یا شمارشی** (مثلاً تعداد دفعات کلمات در متن)  
- کاربرد اصلی: **پردازش زبان طبیعی (NLP)**

فرمول:

$\[
P(C_k | \mathbf{x}) \propto P(C_k) \cdot \prod_{i=1}^n P(x_i | C_k)^{x_i}
\]$

**مثال عملی:**  
ایمیل جدید با تعداد کلمات ["رایگان"=1, "پول"=1, "برنده"=0]  
→ محاسبه احتمال شرطی هر کلمه → ضرب در Prior → انتخاب کلاس با بیشترین احتمال.

---

### ۴.۳ الگوریتم Bernoulli Naive Bayes
- برای داده‌های **دودویی (۰ یا ۱)**، مثل وجود یا عدم وجود کلمه  
- فرمول برای ویژگی دودویی \( x_i \):

$\[
P(x_i | C_k) =
\begin{cases} 
P(x_i=1 | C_k) & \text{اگر وجود دارد} \\
1 - P(x_i=1 | C_k) & \text{اگر وجود ندارد}
\end{cases}
\]$

**مثال عملی:**  
ایمیل جدید: ["رایگان"=1, "پول"=0, "برنده"=0]  
- محاسبه احتمال هر ویژگی با توجه به وجود یا عدم وجود آن  
- ضرب احتمال‌ها و انتخاب کلاس با بیشترین مقدار

---

## ۵. مثال کامل مرحله به مرحله – تشخیص بیماری

| تب | سرفه | سردرد | بیماری؟ |
|----|-------|--------|---------|
| بالا | بله | بله | دارد |
| پایین | خیر | بله | ندارد |
| بالا | بله | خیر | دارد |
| پایین | بله | خیر | ندارد |
| بالا | خیر | بله | دارد |
| پایین | خیر | خیر | ندارد |

نمونه جدید: تب=بالا، سرفه=بله، سردرد=بله  

### مرحله ۱: احتمال پیشین
$\[
P(\text{دارد}) = 0.5, \quad P(\text{ندارد}) = 0.5
\]$

### مرحله ۲: احتمال شرطی (Likelihood)

#### کلاس "دارد":
$\[
P(\text{تب=بالا} \mid \text{دارد}) = \frac{3}{3} = 1
\]$ 

$\[
P(\text{سرفه=بله} \mid \text{دارد}) = \frac{2}{3}
\]$  

$\[
P(\text{سردرد=بله} \mid \text{دارد}) = \frac{2}{3}
\]$  

ضرب در Prior:

$\[
0.5 \cdot 1 \cdot 2/3 \cdot 2/3 \approx 0.222
\]$

#### کلاس "ندارد" (با Laplace Smoothing):
$\[
P(\text{تب=بالا} \mid \text{ندارد}) = \frac{0+1}{3+2} = 0.2
\]$ 

$\[
P(\text{سرفه=بله} \mid \text{ندارد}) = \frac{1+1}{5} = 0.4
\]$ 

$\[
P(\text{سردرد=بله} \mid \text{ندارد}) = \frac{1+1}{5} = 0.4
\]$  

ضرب در Prior:

$\[
0.5 \cdot 0.2 \cdot 0.4 \cdot 0.4 = 0.016
\]$

### مرحله ۳: مقایسه
$\[
0.222 > 0.016 \quad \Rightarrow \text{پیش‌بینی: دارد}
\]$

---

---

# راه حل Laplace Smoothing و مشکل صفر شدن احتمال در Naive Bayes

## ۱. مشکل صفر شدن احتمال (Zero Frequency Problem)
در الگوریتم **Naive Bayes**، احتمال شرطی هر ویژگی (مانند یک کلمه) نسبت به یک کلاس به‌صورت زیر محاسبه می‌شود:

P(x_i | C_k) =
(تعداد مشاهده x_i در کلاس C_k) /
(کل نمونه‌های کلاس C_k)

اگر یک ویژگی در داده‌های آموزشی یک کلاس **هرگز مشاهده نشده باشد**، احتمال آن برابر صفر خواهد شد.

### مثال
فرض کنید پیام زیر را می‌خواهیم طبقه‌بندی کنیم:

"lunch money money money money"

اگر کلمه **lunch** در داده‌های آموزشی اسپم وجود نداشته باشد:

P(lunch | Spam) = 0

از آنجا که Naive Bayes احتمال نهایی را از **ضرب احتمال کلمات** به‌دست می‌آورد، وجود حتی یک احتمال صفر باعث می‌شود کل احتمال برابر صفر شود.  
در نتیجه، پیام حتی با وجود کلمات رایج اسپم، ممکن است اشتباه طبقه‌بندی شود.

---

## ۲. راهکار: Laplace Smoothing  (یا Alpha) 
برای جلوگیری از صفر شدن احتمال‌ها، از **Laplace Smoothing** استفاده می‌شود.  
در این روش، مقدار ثابتی به نام **α** (معمولاً برابر با ۱) به شمارش تمام ویژگی‌ها اضافه می‌شود.

### فرمول Laplace Smoothing

P(x_i | C_k) =
(تعداد مشاهده x_i + 1) /
(کل نمونه‌های کلاس C_k + V)

که در آن:
- V : تعداد کل ویژگی‌ها یا کلمات ممکن (Vocabulary)

### مثال
اگر کلمه **lunch** در داده‌های آموزشی اسپم صفر بار دیده شده باشد، پس از اعمال smoothing خواهیم داشت:

P(lunch | Spam) =
(0 + 1) / (تعداد کل کلمات اسپم + V)
≈ 1 / 11 ≈ 0.09

به این ترتیب:
- هیچ احتمالی صفر نمی‌شود
- پیام «lunch money money money money» دیگر احتمال صفر نخواهد داشت
- و در نهایت به‌درستی به‌عنوان **اسپم** طبقه‌بندی می‌شود

نکته:
اعمال Smoothing، احتمال‌های اولیه (Prior) مانند  
P(Spam) و P(Ham)  
را تغییر نمی‌دهد، زیرا تعداد کل پیام‌ها در مجموعه آموزشی ثابت است.

## **چرا Naive Bayes، "ساده‌لوحانه" است؟**

این الگوریتم "ساده‌لوحانه" (Naive) نامیده می‌شود زیرا:

1.  **نادیده گرفتن ترتیب کلمات:** امتیاز کلمات "dear friend" و "friend dear" دقیقاً یکسان محاسبه می‌شود. ترتیب کلمات نادیده گرفته می‌شود.
2.  **مدل "کیسه کلمات" (Bag of Words):** Naive Bayes با زبان طوری برخورد می‌کند که انگار فقط یک "کیسه پر از کلمات" است و قواعد گرامری و عبارات رایج را نادیده می‌گیرد.
3.  **کارایی:** با وجود این ساده‌سازی (که در اصطلاح یادگیری ماشین، **High Bias** نامیده می‌شود)، Naive Bayes در عمل برای وظایفی مانند فیلتر کردن اسپم بسیار خوب عمل می‌کند (**Low Variance**).

---

 ## ۶. مزایا و معایب Naive Bayes

### مزایا:
- سریع و ساده  
- نیاز به داده کمی دارد  
- عملکرد عالی در طبقه‌بندی متن  
- مقاوم در برابر Overfitting  

### معایب:
- فرض استقلال ویژگی‌ها اغلب نادرست است  
- احتمال صفر با داده کم → نیاز به **Laplace Smoothing**  
- احتمال‌های واقعی را تخمین نمی‌زند، فقط برای **طبقه‌بندی** مناسب است
- 

  


  # جمع‌بندی

1. محاسبه احتمال پیشین \( P(C_k) \)  
2. محاسبه احتمال شرطی \( P(x_i|C_k) \) با smoothing  
3. ضرب احتمال‌ها و Prior برای هر کلاس  
4. انتخاب کلاس با بیشترین مقدار → **پیش‌بینی نهایی**

> این الگوریتم به راحتی قابل توسعه برای هر تعداد ویژگی و کلاس است.
